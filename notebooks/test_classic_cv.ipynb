{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2 as cv\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge print into one folder:\n",
    "\n",
    "roots = [\n",
    "    \"data_digits/hand\",\n",
    "    \"data_digits/print\",\n",
    "] \n",
    "\n",
    "for root in roots:\n",
    "    root = Path(root)\n",
    "\n",
    "    for d in range(1, 10):\n",
    "        (root / str(d)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    files = root.rglob('*.jpg')\n",
    "\n",
    "    for file in files:\n",
    "        parent = file.parent.name\n",
    "\n",
    "        file.rename(root / parent / file.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08285867195867874, 0.2569625918691173)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean and std\n",
    "\n",
    "files = list(Path(\"data_digits/\").rglob('*.jpg'))\n",
    "print(len(files))\n",
    "\n",
    "mean = 0\n",
    "std = 0\n",
    "\n",
    "for file in files:\n",
    "    img = cv.imread(file, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    mean += img.mean()\n",
    "    std += img.std()\n",
    "\n",
    "mean /= len(files)\n",
    "std /= len(files)\n",
    "\n",
    "mean / 255, std / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have 1790\n",
      "Have 1927\n"
     ]
    }
   ],
   "source": [
    "# split on train and test:\n",
    "roots = [\n",
    "    \"data_digits/hand\",\n",
    "    \"data_digits/print\",\n",
    "]\n",
    "\n",
    "def create_split_cls_folders(root: Path, splits, classes):\n",
    "    for split in splits:\n",
    "        for cls in classes:\n",
    "            (root / split / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "splits = ['train', 'test']\n",
    "classes = [str(x) for x in range(1, 10)]\n",
    "\n",
    "for root in roots:\n",
    "    root = Path(root)\n",
    "    files = list(root.rglob('*.jpg'))\n",
    "    print(f\"Have {len(files)}\")\n",
    "\n",
    "    create_split_cls_folders(root, splits, classes)\n",
    "\n",
    "    train_files, test_files = train_test_split(files, test_size=0.10, random_state=1)\n",
    "\n",
    "    for file in train_files:\n",
    "        cls = file.parent.name\n",
    "        file.rename(root / 'train' / cls / file.name)\n",
    "\n",
    "    for file in test_files:\n",
    "        cls = file.parent.name\n",
    "        file.rename(root / 'test' / cls / file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [x for x in Path('data_digits').rglob('*') if x.is_dir()]\n",
    "\n",
    "for f in folders:\n",
    "    if len(list(f.glob('*'))) == 0:\n",
    "        f.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.Normalize((0.082), (0.25696)), # Normalize to mean=0.1307, std=0.3081 (MNIST stats)\n",
    "    #transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.15, 0.15), scale=(0.9, 1.1))\n",
    "    #transforms.RandomPerspective(0.1, p=0.25),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.Normalize((0.082), (0.25696)), # Normalize to mean=0.1307, std=0.3081 (MNIST stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_hand = torchvision.datasets.ImageFolder(root='./data_digits/hand/train', transform=transform)\n",
    "test_dataset_hand = torchvision.datasets.ImageFolder(root='./data_digits/hand/test', transform=transform_test)\n",
    "train_dataset_print = torchvision.datasets.ImageFolder(root='./data_digits/print/train', transform=transform)\n",
    "test_dataset_print = torchvision.datasets.ImageFolder(root='./data_digits/print/test', transform=transform_test)\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset((train_dataset_hand, train_dataset_print))\n",
    "test_dataset = torch.utils.data.ConcatDataset((test_dataset_hand, test_dataset_print))\n",
    "\n",
    "# train_dataset = train_dataset_hand\n",
    "# test_dataset = test_dataset_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3345, 372)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_params(model: nn.Module):\n",
    "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mobilenetv2_050.lamb_in1k',\n",
       " 'mobilenetv2_100.ra_in1k',\n",
       " 'mobilenetv2_110d.ra_in1k',\n",
       " 'mobilenetv2_120d.ra_in1k',\n",
       " 'mobilenetv2_140.ra_in1k']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.list_models('mobilenetv2*', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2234825\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "# model = timm.create_model('mobilenetv3_small_050.lamb_in1k', pretrained=True, num_classes=9, in_chans=1)\n",
    "# print(calculate_params(model)) #577161\n",
    "\n",
    "# model = timm.create_model('resnet10t.c3_in1k', pretrained=True, num_classes=9, in_chans=1)\n",
    "# print(calculate_params(model))\n",
    "\n",
    "model = timm.create_model('mobilenetv2_100.ra_in1k', pretrained=True, num_classes=9, in_chans=1)\n",
    "print(calculate_params(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefaultCfg(tags=deque(['lamb_in1k']), cfgs={'lamb_in1k': PretrainedCfg(url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_small_050_lambc-4b7bbe87.pth', file=None, state_dict=None, hf_hub_id='timm/', hf_hub_filename=None, source=None, architecture=None, tag=None, custom_load=False, input_size=(3, 224, 224), test_input_size=None, min_input_size=None, fixed_input_size=False, interpolation='bicubic', crop_pct=0.875, test_crop_pct=None, crop_mode='center', mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), num_classes=1000, label_offset=None, label_names=None, label_descriptions=None, pool_size=(7, 7), test_pool_size=None, first_conv='conv_stem', classifier='classifier', license=None, description=None, origin_url=None, paper_name=None, paper_ids=None, notes=None)}, is_pretrained=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.models.mobilenetv3 import default_cfgs\n",
    "default_cfgs['mobilenetv3_small_050']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Standard loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = 'cuda'\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients in evaluation\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.9571\n",
      "Test Accuracy: 73.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1370: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 0.3311\n",
      "Test Accuracy: 95.43%\n",
      "Epoch [3/50], Loss: 0.1992\n",
      "Test Accuracy: 98.39%\n",
      "Epoch [4/50], Loss: 0.1226\n",
      "Test Accuracy: 99.46%\n",
      "Epoch [5/50], Loss: 0.1266\n",
      "Test Accuracy: 98.66%\n",
      "Epoch [6/50], Loss: 0.0680\n",
      "Test Accuracy: 99.19%\n",
      "Epoch [7/50], Loss: 0.0926\n",
      "Test Accuracy: 99.46%\n",
      "Epoch [8/50], Loss: 0.1118\n",
      "Test Accuracy: 99.19%\n",
      "Epoch [9/50], Loss: 0.0603\n",
      "Test Accuracy: 98.12%\n",
      "Epoch [10/50], Loss: 0.0714\n",
      "Test Accuracy: 99.46%\n",
      "Epoch [11/50], Loss: 0.1150\n",
      "Test Accuracy: 99.46%\n",
      "Epoch [12/50], Loss: 0.0907\n",
      "Test Accuracy: 99.46%\n",
      "Epoch [13/50], Loss: 0.1189\n",
      "Test Accuracy: 97.85%\n",
      "Epoch [14/50], Loss: 0.2032\n",
      "Test Accuracy: 97.31%\n",
      "Epoch [15/50], Loss: 0.0971\n",
      "Test Accuracy: 99.19%\n",
      "Epoch [16/50], Loss: 0.0837\n",
      "Test Accuracy: 99.73%\n",
      "Epoch [17/50], Loss: 0.0489\n",
      "Test Accuracy: 99.46%\n",
      "Epoch [18/50], Loss: 0.0465\n",
      "Test Accuracy: 99.19%\n",
      "Epoch [19/50], Loss: 0.0399\n",
      "Test Accuracy: 99.73%\n",
      "Epoch [20/50], Loss: 0.0371\n",
      "Test Accuracy: 99.73%\n",
      "Epoch [21/50], Loss: 0.0407\n",
      "Test Accuracy: 100.00%\n",
      "Epoch [22/50], Loss: 0.0508\n",
      "Test Accuracy: 100.00%\n",
      "Epoch [23/50], Loss: 0.0660\n",
      "Test Accuracy: 100.00%\n",
      "Epoch [24/50], Loss: 0.0340\n",
      "Test Accuracy: 99.73%\n",
      "Epoch [25/50], Loss: 0.0176\n",
      "Test Accuracy: 100.00%\n",
      "Epoch [26/50], Loss: 0.0243\n",
      "Test Accuracy: 99.73%\n",
      "Epoch [27/50], Loss: 0.0245\n",
      "Test Accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m      9\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero gradients from previous step\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/Image.py:984\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    982\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 50  # You can increase this for better accuracy\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero gradients from previous step\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    test_score = test_model(model, test_loader)\n",
    "\n",
    "    if test_score > best_score:\n",
    "        best_score = test_score\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    scheduler.step(running_loss, epoch=epoch)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNormAct2d(\n",
       "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): ReLU6(inplace=True)\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNormAct2d(\n",
       "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (bn2): BatchNormAct2d(\n",
       "          960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (aa): Identity()\n",
       "        (se): Identity()\n",
       "        (conv_pwl): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNormAct2d(\n",
       "          320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "          (drop): Identity()\n",
       "          (act): Identity()\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNormAct2d(\n",
       "    1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "    (drop): Identity()\n",
       "    (act): ReLU6(inplace=True)\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (classifier): Linear(in_features=1280, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"data_digits/hand/test\")\n",
    "\n",
    "for digit in range(1, 10):\n",
    "    (root / str(digit)).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "root = Path(\"data_digits/hand\")\n",
    "digits_files = list(root.glob('*.jpg'))\n",
    "\n",
    "\n",
    "for file in digits_files:\n",
    "    digit = cv.imread(file, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "    inp = transform_test(digit).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(inp.unsqueeze(0))\n",
    "        _, pred = torch.max(output, 1)\n",
    "\n",
    "    #viz(digit)\n",
    "    #print(test_dataset.classes[pred])\n",
    "    file.rename(file.parent / \"test\" / train_dataset.classes[pred] / file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"data_digits/hand\")\n",
    "\n",
    "for digit in range(1, 10):\n",
    "    (root / str(digit)).mkdir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sudoku",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
